# 0914
```bash
## kill 명령어
# 킬 시그널 확인 가능
kill -l

# 프로세스를 지정해서 kill 가능
kill -9 PID

linux kill   ->   kill -9 PID
=================================
docker kill  ->  forceful shutdown -> 9 SIGKILL
docker stop  ->  graceful shutdown -> 15 SIGTERM

```

이미지를 백업한다 ⇒ save  

백업한 이미지를 가져온다 ⇒ load  
<br>

Q. 왜 backup.tar을 사용했을까?

A. 이미지는 계층 구조이기 때문에 tar를 통해 하나로 묶어야 한다.

Q. save vs export

A. 이미지는 정적이고 컨테이너는 동적인 상태이다. 정적인 상태에서 백업하는 게 보통이므로 이전/백업할 때 되도록 save를 사용

![image (1)](https://user-images.githubusercontent.com/54930365/191781502-9f5a8586-29ef-4501-a36f-54aa2b82d743.png)

export 받은 파일을 import 시키면 반드시 도커 파일을 통해 한단계를 추가로 거쳐줘야 사용가능하다.

그러나 save/load를 사용하면 곧바로 사용 가능하다.

```bash
## Docker Container 관리 - Container Migration

# webserver Container를 webserver.tar 파일로 저장
docker run --name webserver -d -p 9999:80 nginx 1.23.1
docker export webserver > webserver.tar
# 생성된 tar 파일 상세 확인
tar -tvf webserver.tar

sudo scp webserver.tar kevin@hostos2:/home/kevin/backup/webserver.tar

==================================================================
# 2번 서버에서 webserver.tar 파일을 기반으로 myweb:3.0 image를 생성
mkdir backup
cat webserver.tar | docker import - myweb:3.0

docker images

docker run -d --name=myweb3 -p 8002:80 myweb:3.0
=> 에러 발생. run이 동작하지 않음. 
왜? 백업에서 가져왔기 때문에 도커 파일로 추가 동작을 설정해줘야 run이 동작한다.

```
<br>  

```bash
## Docker Container 관리 - image backup & migration 시 유용

mkdir save_lab && cd $_
docker image save phpserver:1.0 > phpserver1.tar
ls -lh

docker image save phpserver:1.0 | gzip > phpserver1.tar.gz
ls -lh

docker image save phpserver:1.0 | bzip2 > phpserver1.tar.bz2
ls -lh

scp phpserver1.tar.gz kevin@hostos2:/home/kevin/backup/phpserver1.tar.gz

=======================================================================
docker image load < phpserver1.tar.gz
docker images
docker run -itd -p 8200:80 phpserver:1.0
curl localhost

```

<키워드>

스위치 오버와 페일오버

IPtables

full duplex link(전이중 통신)
CNM(Container Network Model)

브릿지 네트워크는 Mac Address 기반의 통신

Linux Network Building Block

도커 네트워크는 리눅스 네트워크와 같다.

brctl show

⇒ bridge control, 브릿지에 어떤 이더넷이 붙었는지

docker exec -it myos5 ip addr

docker exec -it myos5 route

docker inspect myos5 | grep Mac

cat /sys/class/net/veth93efcbc/ifindex
cat /sys/class/net/veth93efcbc/iflink

docker exec -it myos9 cat /sys/class/net/eth0/iflink

docker run -it --name=myos11 --add-host=hostos1:192.168.56.101 centos:7

docker run -it --name=myos12 --dns=8.8.8.8 centos:7 cat/etx/resolv.conf nameserver 8.8.8.8

docker run -d --name=webserver2 -P nginx:1.23.1-alpine
docker port webserver2

docker run -d —name=webserver3 —expose=10000 -P nginx:1.23.1-alpine

docker port webserver3

iptraf-ng

⇒ 네트워크 패킷 사용량 확인,

netstat

⇒ 네트워크 접속, 라우팅 테이블, 네트워크 인터페이스의 통계 정보를 보여주는 도구

docker run -d —name=hostnet —net=host nginx:1.23.1-alpine

⇒ —het=host를 사용하면 ip가 없다. 또한 host에 붙기 때문에 host의 포트 번호를 사용하게 된다.

host 방식으로 컨테이너를 생성하면, 컨테이너가 독립적인 네트워크 영역을 갖지 않고 host 네트워크를 함께 사용하게 된다. (네트워크 외 다른 환경은 기존과 동일하다)

[출처](https://bluese05.tistory.com/38)

[도커 네트워크 자료](https://youngmind.tistory.com/entry/%EB%8F%84%EC%BB%A4-%EA%B0%95%EC%A2%8C-3-%EB%8F%84%EC%BB%A4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-1) [https://jonnung.dev/docker/2020/02/16/docker_network/](https://jonnung.dev/docker/2020/02/16/docker_network/)4

[https://linux.systemv.pe.kr/docker-네트워크-이해/](https://linux.systemv.pe.kr/docker-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EC%9D%B4%ED%95%B4/)

```bash
##
* 조회: route / ifconfig / docker network ls / brctl show

# T1
docker network create -d bridge web-net
route
docker network ls
ifconfig

# T2
docker run --net=web-net -it --name=net-check1 ubuntu:14.04 bash
ifconfig
route

# T3
dokcker run --net=web-net -it --name=net-check2 ubuntu:14.04 bash

# 다른 터미널에서
brctl show

docker network inspect web-net
-> "Subnet": "172.18.0.0/16" -> 65536개
```

Cloud나 Container를 사용 시 CIDR 기법으로 IP 대역을 지정

CIDR은 Private network를 지향하겠다는 의도!

보통 기업들이 사용하는 Private network 대역은 3가지 형태가 있다.(RFC 1918 국제표준 권고안)

1) 10.0.0.0 ~ 10.255.255.255 → 10.0.0.0/8  → 1670만개

2) 172.16.0. 0 ~ 172.31.255.255 → 172.16.0.0/12 → 100만개

3) 192.168.0.0 ~ 192.168.255.255 → 192.168.0.0/16

```bash

docker network create \
--driver bridge \
--subnet 172.30.1.0/24 \      -> CIDR 표기만 설정 가능, 255.255.255.0과 같음
--ip-range 172.30.1.0/24 \    -> subnet 이하, IP 범위 조정 가능(172.30.1.100/26)
--gateway 172.30.1.1 \        
vswitch-net                    -> 256개의 IP 중 254개 서용 가능

docker network ls

docker run --net=vswitch-net -itd --name=net1 ubuntu:14.04
# 해당 대역의 IP 지정은 --ip x.x.x.x 옵션 사용
docker run --net=vswitch-net -itd --name=net2 --ip 172.30.1.100 ubuntu:14.04

# 생성된 브릿지의 정보를 inspect 명령을 통해 확인
docker network inspect vswitch-net

docker inspect net1 | grep IPAddress
docker inspect net2 | grep IPAddress
brctl show       -> new bridge network에 연결된 vethxxx 확인
route
docker exec net1 ip a

```

container load balancer = switch 장비 와 같은 역할

→ proxy → HAproxy, Nginx, Apache LB

1) docker container self LB → 내장된 DNS 서버(서비스)로 구현

→ 사용자 정의 Bridge network 생성

→ —net-alias 옵션 ⇒ target group(workload(트래픽)를 받을 서버(컨테이너)의 그룹)

→ 자체 DNS 서비스 활성화 → 127.0.0.11 → docker DNS = Service Discovery

2) nginx container를 proxy로 전환하여 LB로 구성

외부 —— [nginx LB]  —- c1

                                  —- c2

                                  —- c3

3) [LAB]
```shell
## 컨테이너 로드밸런서

docker network create \
--driver bridge \
--subnet 172.200.1.0/24 \
--ip-range 172.200.1.0/24 \
--gateway 172.200.1.1 \
netlb

docker network ls
route
docker run -itd --name=lb-test1 --net=netlb --net-alias=tg-net ubuntu:14.04
docker run -itd --name=lb-test2 --net=netlb --net-alias=tg-net ubuntu:14.04
docker run -itd --name=lb-test3 --net=netlb --net-alias=tg-net ubuntu:14.04
docker inspect lb-test1 | grep IPA
docker inspect lb-test2 | grep IPA
docker inspect lb-test3 | grep IPA

docker run -it --name=frontend --net=netlb ubuntu:14.04 bash

## RR이 아닌 랜점으로 응답하는 컨테이너 확인
ping -c 2 tg-net

apt -y update
apt -y install dnsutils
dig tg-net

# 2번 터미널
docker run -itd --name=lb-test4 --net=netlb --net-alias=tg-net ubuntu:14.04

# 1번 터미널
# 추가된 컨테이너가 자동으로 DNS에 등록됨을 확인
dig tg-net
# 추가된 lb-test4도 png 요청에 응답(ANSWER)
ping -c 2 tg-net
=> 172.200.1.2 ~ 172.200.1.6 중 로드밸런싱되어 ping이 보내진다.
```  
<br>

```shell
## 컨테이너 로드밸런서

docker network create \
--driver bridge \
--subnet 172.200.1.0/24 \
--ip-range 172.200.1.0/24 \
--gateway 172.200.1.1 \
netlb

docker network ls
route
docker run -itd --name=lb-test1 --net=netlb --net-alias=tg-net ubuntu:14.04
docker run -itd --name=lb-test2 --net=netlb --net-alias=tg-net ubuntu:14.04
docker run -itd --name=lb-test3 --net=netlb --net-alias=tg-net ubuntu:14.04
docker inspect lb-test1 | grep IPA
docker inspect lb-test2 | grep IPA
docker inspect lb-test3 | grep IPA

docker run -it --name=frontend --net=netlb ubuntu:14.04 bash

## RR이 아닌 랜점으로 응답하는 컨테이너 확인
ping -c 2 tg-net

apt -y update
apt -y install dnsutils
dig tg-net

# 2번 터미널
docker run -itd --name=lb-test4 --net=netlb --net-alias=tg-net ubuntu:14.04

# 1번 터미널
# 추가된 컨테이너가 자동으로 DNS에 등록됨을 확인
dig tg-net
# 추가된 lb-test4도 png 요청에 응답(ANSWER)
ping -c 2 tg-net
=> 172.200.1.2 ~ 172.200.1.6 중 로드밸런싱되어 ping이 보내진다.
```  
<br>  

```shell
## 컨테이너 로드밸런서

docker network create \
--driver bridge \
--subnet 172.200.1.0/24 \
--ip-range 172.200.1.0/24 \
--gateway 172.200.1.1 \
netlb

docker network ls
route
docker run -itd --name=lb-test1 --net=netlb --net-alias=tg-net ubuntu:14.04
docker run -itd --name=lb-test2 --net=netlb --net-alias=tg-net ubuntu:14.04
docker run -itd --name=lb-test3 --net=netlb --net-alias=tg-net ubuntu:14.04
docker inspect lb-test1 | grep IPA
docker inspect lb-test2 | grep IPA
docker inspect lb-test3 | grep IPA

docker run -it --name=frontend --net=netlb ubuntu:14.04 bash

## RR이 아닌 랜점으로 응답하는 컨테이너 확인
ping -c 2 tg-net

apt -y update
apt -y install dnsutils
dig tg-net

# 2번 터미널
docker run -itd --name=lb-test4 --net=netlb --net-alias=tg-net ubuntu:14.04

# 1번 터미널
# 추가된 컨테이너가 자동으로 DNS에 등록됨을 확인
dig tg-net
# 추가된 lb-test4도 png 요청에 응답(ANSWER)
ping -c 2 tg-net
=> 172.200.1.2 ~ 172.200.1.6 중 로드밸런싱되어 ping이 보내진다.
```